{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManJ-PC/redes-neurais/blob/master/vetoresDePalavras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVZpMYNYo2sk"
      },
      "source": [
        "# Palavras Embutidas ou Vetores de Palavras (Word Embeddings)\n",
        "Adaptado do livro do Chollet (Cap. 6.1: [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff)).\n",
        "\n",
        "Este código mostra uma outra maneira para representar palavras e continua no mesmo exemplo de análise de sentimentos do código apresentado neste vídeo: [Análise de Sentimento com Keras](https://youtu.be/fG706DT0dOc)\n",
        "\n",
        "\n",
        "Este vídeo explica o que são e as vantagens de se usar a representação por palavras embutidas (word embeddings): [Word Embeddings, Palavras Embutidas ou Vetores de Palavras](https://youtu.be/Gli2fxRtQlg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ7Ui7Rk7uqn"
      },
      "source": [
        "### Rodando este código localmente  \n",
        "\n",
        "Como este exemplo envolve dois arquivos muito grandes que terão que ser baixados da Internet e descompactados, ao invés de executar este notebook jupyter usando o servidor remoto da google, iremos executá-lo localmente. No lado superior esquerdo da tela do Colab tem um botão que você pode usar para trocar do modo remoto (hosted runtime) para o modo local (local runtime). Se sua máquina já não tiver preparada para rodar o keras e o jupyter tente seguir as inscruções abaixo que eu usei para instalar em uma máquina com Ubuntu 18.04 e GPU Nvidia GTX 1070 (e boa sorte pois você vai precisar !!!):\n",
        "\n",
        "#### Instalando Tensorflow no Ubuntu 18.04\n",
        "\n",
        "Eu segui as instruções disponíveis aqui (usando miniconda):\n",
        "[tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip](https://towardsdatascience.com/tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip-52e5249374bc)\n",
        "\n",
        "#### Instalando Keras\n",
        "conda activate tf_gpu\n",
        "\n",
        "conda install keras\n",
        "\n",
        "#### Instalando jupyter localmente\n",
        "Eu segui as instruções disponíveis aqui (use conda ao invés de pip se não quiser ter mais dor de cabeça do que o necessário):\n",
        "[local-runtimes](https://research.google.com/colaboratory/local-runtimes.html)\n",
        "\n",
        "#### Instalando o matplotlib\n",
        "conda install matplotlib\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1OkUtFo2so"
      },
      "source": [
        "##Aprendendo Vetores de Palavras a partir de Dados\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cav2f5b1o2st"
      },
      "source": [
        "### Carregando o IMDB\n",
        "A seguir o banco de treinamento e teste é carregado e ajustado para um formato aceito como entrada para a rede neural. Notem que as sentenças vão ser cortadas nas primeiras 20 palavras (é um parâmetro importante neste caso e se perderá informação).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaVqWryEo2sv",
        "outputId": "b880c13c-053f-4810-dbc4-39bc00507ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import keras\n",
        "keras.__version__\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "# Quantidade de palavras do dicionário\n",
        "max_features = 10000\n",
        "# Tamanho máximo das sentenças\n",
        "maxlen = 20\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Carregando o conjunto de treinamento e teste IMDB\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "\n",
        "# Convertendo para um formato que a rede neural aceita, cortando cada sentença\n",
        "# para um tamanho fixo = maxlen\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
            "   15   16 5345   19  178   32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7OLNz0rOa1o"
      },
      "source": [
        "### Criando o modelo e treinando\n",
        "\n",
        "Novamente uma arquitetura sequencial em camadas é criada, mas agora com uma camada de \"Embedding\" . O parâmetro 10000 é o total de palavras do seu vocabulário que também é o tamanho da camada de entrada com cada palavra sendo representada usando a codificação uma-quente (one-hot encoding). O parâmetro 8 é o tamanho do vetor para a nova representação que será aprendida. Assista ao vídeo sobre palavras embutidas para entender os conceitos de one-hot encoding e word embedding:  [Word Embeddings, Palavras Embutidas ou Vetores de Palavras](https://youtu.be/Gli2fxRtQlg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZivf24eo2s0",
        "outputId": "927605d2-2932-41c8-9309-94dd3989f9f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 10000 = total de palavras no dicionário, 8 = tamanho do vetor embutido,\n",
        "# maxlen = tamanho máximo, em palavras, de cada sentença\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "\n",
        "# Achata o vetor 3D em um 2D de dimensão \"total de amostras X maxlen*8\"\n",
        "model.add(Flatten())\n",
        "\n",
        "# A ultima camada é uma completamente conectada (Dense)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "# Note no código abaixo que diferentemente do exemplo anterior, o conjunto de\n",
        "# validação e teste é gerado automaticamente através do parâmetro validation_split\n",
        "# igual a 0.2 (ou seja, 0.8 ou 80% para treinamento e 0.2 ou 20% para validação)\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0920 17:43:22.117188 139738181203712 deprecation_wrapper.py:119] From /home/pistori/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0920 17:43:22.136806 139738181203712 deprecation_wrapper.py:119] From /home/pistori/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0920 17:43:22.139297 139738181203712 deprecation_wrapper.py:119] From /home/pistori/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0920 17:43:22.286251 139738181203712 deprecation_wrapper.py:119] From /home/pistori/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0920 17:43:22.304802 139738181203712 deprecation_wrapper.py:119] From /home/pistori/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0920 17:43:22.309739 139738181203712 deprecation.py:323] From /home/pistori/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0920 17:43:23.076895 139738181203712 deprecation_wrapper.py:119] From /home/pistori/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bd5d52b3f7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     validation_split=0.2)\n\u001b[0m",
            "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m                 config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n\u001b[1;32m    185\u001b[0m                                         allow_soft_placement=True)\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0m_SESSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \"\"\"\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "380e8-OXo2s5"
      },
      "source": [
        "Veja pela saída do comando model.fit que resultado obtido em acurácia na validação (val_acc)  é pior do que no exemplo anterior, sem word embeddings, mas é porque usamos apenas as primeiras 20 palavras de cada comentários e uma arquitetura mais simples depois da camada de embutimento. Melhorando a arquitetura e ajustando alguns parâmetros dá para melhorar bem. Aqui foi só uma exemplo para entender como usar uma camada de embeddings. Abaixo será criado um outro modelo mas agora usando transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63epnjp1o2s6"
      },
      "source": [
        "## Usando Vetores de Palavras (Word Embeddings) pré-treinados (Transfer Learning)\n",
        "\n",
        "Os códigos a seguir irão utilizar uma representação de vetores de palavras previamente gerada através do algoritmo \"GloVe\" que foi treinado usando todos os textos em Inglês da Wikipedia em 2014. Uma outra alternativa ao GloVe, também muita usada, é o Word2Vec também disponível no Keras.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HTnNyMTo2s9"
      },
      "source": [
        "\n",
        "### Download dos comentários do IMDB original\n",
        "\n",
        "Ao invés de usar a função do Keras que facilita carregar o IMDB já codificado (usando o dicionário de números) o exemplos abaixo carregará os comentários em formato \"bruto\" (raw) como sequência de caracteres. Assim ficará mais fácil reaproveitar os códigos em outros problemas.\n",
        "\n",
        "IMPORTANTE: Você precisa fazer manualmente o download do IMDB, de 80Mb, no site [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/), descompactar em uma pasta na sua máquina e alterar a inicialização da variável \"imdb_dir\" abaixo (/home/pistori/Downloads/aclImdb deve ser trocado pela pasta que foi criada na sua máquina)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uFYKehCo2s-"
      },
      "source": [
        "import os\n",
        "\n",
        "imdb_dir = '/home/pistori/Downloads/aclImdb/'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "# Dê uma olhada na estrutura de pastas do banco que você baixou. Os códigos\n",
        "# abaixo apenas navegam por estas pastas lendo os comentários e colocando na\n",
        "# lista de comentários (texts). # Também monta a lista \"labels\" com a\n",
        "# classificação, positiva ou negativa (1 ou 0) de cada comentário.\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "print(texts[55]) # Mostrando o comentário 55"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mhadxW3o2tC"
      },
      "source": [
        "### Transformar os textos em um vetor de símbolos atômicos (tokens)\n",
        "\n",
        "A atomização (tokenização) consiste em identificar e separar os elementos do texto que devem ser tratados como unidades básicas (atómos). Neste exemplo, os átomos serão as palavras (mas poderiam, em outra situação, ser caracteres ou mesmo sentenças)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1O3Y5rOo2tD"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # Apenas as primeiras 100 palavras do comentário serão usadas\n",
        "training_samples = 1000  # Serão usadas apenas 1000 palavras para treinamento\n",
        "validation_samples = 5000  # e 5000 para validação\n",
        "max_words = 10000  # Tamanho do dicionário de palavras, neste caso, 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Foram encontrados %s símbolos atômicos ou tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Formato do tensor com os comentários:', data.shape)\n",
        "print('Formato do tensor com as classificações:', labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZH5_TJi3dGS"
      },
      "source": [
        "### Aleatorizando e dividindo entre conjunto de treinamento e validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-hyl24S3ckG"
      },
      "source": [
        "# Aleatoriza os dados e depois divide em treinamento e validação\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YcqXCVVo2tJ"
      },
      "source": [
        "### Baixando e processandos as palavras embutidas pré-treinadas com o GloVe\n",
        "\n",
        "Entre no site [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/), baixe o arquivo `glove.6B.zip` e descompacte (é um arquivo com cerca de 800Mb). Ajuste no código abaixo a variável \"glove_dir\" para apontar para a pasta onde você descompactou o arquivo (deve ter um arquivo chamado glove.6B.100d.txt nesta pasta). Será criado uma mapeamento, embeddings_index, onde cada palavra será associada ao seu vetor embutido de 100 dimensões (no arquivo compactado tem também opções para usar vetores de 50, 200, 300 dimensões)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHphsqmho2tL"
      },
      "source": [
        "glove_dir = '/home/pistori/Downloads/glove/'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Total de vetores de palavras encontrados na base Glove = %s' % len(embeddings_index))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8ibea4g_K0Y"
      },
      "source": [
        "print('Vetor embutido correspondente à palavra love = ',embeddings_index['love'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDDlobsao2tQ"
      },
      "source": [
        "Abaixo será construída a matriz (tensor) com as palavras embutidas para cada uma das 10.000 do IMDB que iremos usar. Pode acontecer de uma palavras não ter um vetor embutido no GloVe e neste caso, o vetor ficará todo zerado (pode ser, por exemplo, uma palavras com erro de Inglês ou coisas estranhas que as pessoas escrevem nos comentários, como YAAAAHHHH)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSah-PmTo2tR"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKwgLyeno2tV"
      },
      "source": [
        "Abaixo será construída a rede neural que usaremos para aprender a classificar os comentários (ainda com uma camada de embutimento/embedding 'vazia')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0T03ZRoo2tW"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3AyISh9o2tc"
      },
      "source": [
        "### Colocando as palavras embutidas do GloVe no modelo\n",
        "\n",
        "\n",
        "Os comandos abaixo ajustam os parâmetros da camada de embedding para corresponderem às palavras que baixamos da Internet (GloVe). Note que estamos indicando também que esta camada não deve ser treinada a partir dos dados (trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jxWSMJ9o2td"
      },
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjlko9Qso2th"
      },
      "source": [
        "### Usando o modelo para aprender e avaliando o resultado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpiA0muro2ti"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F48uJnAHo2tn"
      },
      "source": [
        "Mostrando um gráfico do desempenho entre as épocas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCRuJDb4o2tq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Acurácia Treino')\n",
        "plt.plot(epochs, val_acc, 'b', label='Acurácia Val.')\n",
        "plt.title('Acurácia no treinamento e na validação')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Perda Treino')\n",
        "plt.plot(epochs, val_loss, 'b', label='Perda Validação')\n",
        "plt.title('Perda no treinamento e na validação')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bMHO2aQo2t-"
      },
      "source": [
        "A acurácia novamente é menor do que no exemplo anterior, mas dá para melhorar bastante mexendo, por exemplo, no total de exemplos usados para treinamento (bem menos neste caso do que no exemplo anterior)\n",
        "\n",
        "Abaixo temos a avaliação do modelo no conjunto independente de teste. Note que é preciso preparar o conjunto de teste também, como fizemos no treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSSG4HLWo2t_"
      },
      "source": [
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tgTSsc9o2uD"
      },
      "source": [
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}